{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-11T11:14:31.804164Z",
     "start_time": "2025-07-11T11:14:30.997850Z"
    }
   },
   "source": [
    "# app.py\n",
    "import os, json, base64, asyncio, websockets\n",
    "from fastapi import FastAPI, WebSocket, Request\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.websockets import WebSocketDisconnect\n",
    "from twilio.twiml.voice_response import VoiceResponse, Connect\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "app = FastAPI()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PORT = int(os.getenv(\"PORT\", 5050))\n",
    "\n",
    "VOICE = \"alloy\"\n",
    "SYSTEM_MESSAGE = \"You are a helpful, friendly AI assistant. Keep your responses brief and cheerful.\"\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def index():\n",
    "    return \"<h1>gpt-4o-mini Realtime Test Server Running</h1>\"\n",
    "\n",
    "@app.api_route(\"/incoming-call\", methods=[\"GET\", \"POST\"])\n",
    "async def handle_incoming_call(request: Request):\n",
    "    response = VoiceResponse()\n",
    "    host = request.url.hostname\n",
    "    connect = Connect()\n",
    "    connect.stream(url=f\"wss://{host}/media-stream\")\n",
    "    response.append(connect)\n",
    "    return HTMLResponse(content=str(response), media_type=\"application/xml\")\n",
    "\n",
    "@app.websocket(\"/media-stream\")\n",
    "async def handle_media_stream(websocket: WebSocket):\n",
    "    await websocket.accept()\n",
    "\n",
    "    async with websockets.connect(\n",
    "        'wss://api.openai.com/v1/realtime?model=gpt-4o-mini-realtime-preview-2024-12-17',\n",
    "        extra_headers={\n",
    "            \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "            \"OpenAI-Beta\": \"realtime=v1\"\n",
    "        }\n",
    "    ) as openai_ws:\n",
    "        await send_session_update(openai_ws)\n",
    "        stream_sid = None\n",
    "        mark_queue = []\n",
    "        latest_media_timestamp = 0\n",
    "        last_assistant_item = None\n",
    "        response_start_timestamp_twilio = None\n",
    "\n",
    "        async def receive_from_twilio():\n",
    "            nonlocal stream_sid, latest_media_timestamp\n",
    "            try:\n",
    "                async for msg in websocket.iter_text():\n",
    "                    data = json.loads(msg)\n",
    "                    if data['event'] == 'media' and openai_ws.open:\n",
    "                        latest_media_timestamp = int(data['media']['timestamp'])\n",
    "                        await openai_ws.send(json.dumps({\n",
    "                            \"type\": \"input_audio_buffer.append\",\n",
    "                            \"audio\": data['media']['payload']\n",
    "                        }))\n",
    "                    elif data['event'] == 'start':\n",
    "                        stream_sid = data['start']['streamSid']\n",
    "                    elif data['event'] == 'mark' and mark_queue:\n",
    "                        mark_queue.pop(0)\n",
    "            except WebSocketDisconnect:\n",
    "                await openai_ws.close()\n",
    "\n",
    "        async def send_to_twilio():\n",
    "            nonlocal last_assistant_item, response_start_timestamp_twilio\n",
    "            async for msg in openai_ws:\n",
    "                res = json.loads(msg)\n",
    "                if res.get('type') == 'response.audio.delta' and 'delta' in res:\n",
    "                    payload = base64.b64encode(base64.b64decode(res['delta'])).decode()\n",
    "                    await websocket.send_json({\n",
    "                        \"event\": \"media\",\n",
    "                        \"streamSid\": stream_sid,\n",
    "                        \"media\": {\"payload\": payload}\n",
    "                    })\n",
    "\n",
    "                    if response_start_timestamp_twilio is None:\n",
    "                        response_start_timestamp_twilio = latest_media_timestamp\n",
    "\n",
    "                    if res.get('item_id'):\n",
    "                        last_assistant_item = res['item_id']\n",
    "                    await send_mark(websocket, stream_sid)\n",
    "\n",
    "                elif res.get('type') == 'input_audio_buffer.speech_started' and last_assistant_item:\n",
    "                    await handle_interruption()\n",
    "\n",
    "        async def handle_interruption():\n",
    "            nonlocal last_assistant_item, response_start_timestamp_twilio\n",
    "            elapsed = latest_media_timestamp - response_start_timestamp_twilio\n",
    "            await openai_ws.send(json.dumps({\n",
    "                \"type\": \"conversation.item.truncate\",\n",
    "                \"item_id\": last_assistant_item,\n",
    "                \"content_index\": 0,\n",
    "                \"audio_end_ms\": elapsed\n",
    "            }))\n",
    "            await websocket.send_json({\n",
    "                \"event\": \"clear\",\n",
    "                \"streamSid\": stream_sid\n",
    "            })\n",
    "            mark_queue.clear()\n",
    "            last_assistant_item = None\n",
    "            response_start_timestamp_twilio = None\n",
    "\n",
    "        async def send_mark(ws, sid):\n",
    "            if sid:\n",
    "                await ws.send_json({\n",
    "                    \"event\": \"mark\",\n",
    "                    \"streamSid\": sid,\n",
    "                    \"mark\": {\"name\": \"responsePart\"}\n",
    "                })\n",
    "                mark_queue.append(\"responsePart\")\n",
    "\n",
    "        await asyncio.gather(receive_from_twilio(), send_to_twilio())\n",
    "\n",
    "async def send_session_update(openai_ws):\n",
    "    await openai_ws.send(json.dumps({\n",
    "        \"type\": \"session.update\",\n",
    "        \"session\": {\n",
    "            \"turn_detection\": {\"type\": \"server_vad\"},\n",
    "            \"input_audio_format\": \"g711_ulaw\",\n",
    "            \"output_audio_format\": \"g711_ulaw\",\n",
    "            \"voice\": VOICE,\n",
    "            \"instructions\": SYSTEM_MESSAGE,\n",
    "            \"modalities\": [\"text\", \"audio\"],\n",
    "            \"temperature\": 0.6\n",
    "        }\n",
    "    }))\n",
    "\n",
    "    await openai_ws.send(json.dumps({\n",
    "        \"type\": \"conversation.item.create\",\n",
    "        \"item\": {\n",
    "            \"type\": \"message\",\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"input_text\",\n",
    "                \"text\": \"Hello! You're now connected to a gpt-4o-mini voice assistant. Feel free to ask anything.\"\n",
    "            }]\n",
    "        }\n",
    "    }))\n",
    "    await openai_ws.send(json.dumps({\"type\": \"response.create\"}))\n",
    "\n",
    "# Run server\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=PORT)\n"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 147\u001B[39m\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    146\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01muvicorn\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m147\u001B[39m     \u001B[43muvicorn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mapp:app\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhost\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m0.0.0.0\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m=\u001B[49m\u001B[43mPORT\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\anaconda3\\envs\\ollama\\Lib\\site-packages\\uvicorn\\main.py:580\u001B[39m, in \u001B[36mrun\u001B[39m\u001B[34m(app, host, port, uds, fd, loop, http, ws, ws_max_size, ws_max_queue, ws_ping_interval, ws_ping_timeout, ws_per_message_deflate, lifespan, interface, reload, reload_dirs, reload_includes, reload_excludes, reload_delay, workers, env_file, log_config, log_level, access_log, proxy_headers, server_header, date_header, forwarded_allow_ips, root_path, limit_concurrency, backlog, limit_max_requests, timeout_keep_alive, timeout_graceful_shutdown, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_version, ssl_cert_reqs, ssl_ca_certs, ssl_ciphers, headers, use_colors, app_dir, factory, h11_max_incomplete_event_size)\u001B[39m\n\u001B[32m    578\u001B[39m         Multiprocess(config, target=server.run, sockets=[sock]).run()\n\u001B[32m    579\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m580\u001B[39m         \u001B[43mserver\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    581\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[32m    582\u001B[39m     \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# pragma: full coverage\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\anaconda3\\envs\\ollama\\Lib\\site-packages\\uvicorn\\server.py:67\u001B[39m, in \u001B[36mServer.run\u001B[39m\u001B[34m(self, sockets)\u001B[39m\n\u001B[32m     65\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m, sockets: \u001B[38;5;28mlist\u001B[39m[socket.socket] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     66\u001B[39m     \u001B[38;5;28mself\u001B[39m.config.setup_event_loop()\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43masyncio\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mserve\u001B[49m\u001B[43m(\u001B[49m\u001B[43msockets\u001B[49m\u001B[43m=\u001B[49m\u001B[43msockets\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\anaconda3\\envs\\ollama\\Lib\\asyncio\\runners.py:191\u001B[39m, in \u001B[36mrun\u001B[39m\u001B[34m(main, debug, loop_factory)\u001B[39m\n\u001B[32m    161\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Execute the coroutine and return the result.\u001B[39;00m\n\u001B[32m    162\u001B[39m \n\u001B[32m    163\u001B[39m \u001B[33;03mThis function runs the passed coroutine, taking care of\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    187\u001B[39m \u001B[33;03m    asyncio.run(main())\u001B[39;00m\n\u001B[32m    188\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    189\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m events._get_running_loop() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    190\u001B[39m     \u001B[38;5;66;03m# fail fast with short traceback\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m191\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    192\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33masyncio.run() cannot be called from a running event loop\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    194\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001B[38;5;28;01mas\u001B[39;00m runner:\n\u001B[32m    195\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m runner.run(main)\n",
      "\u001B[31mRuntimeError\u001B[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "96e1c9be94947e97"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
